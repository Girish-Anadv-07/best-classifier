{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=\"center\"><font size=\"5\">Classification with Python</font></h1>","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"We load a dataset using Pandas library, and apply the following algorithms, and find the best one for this specific dataset by accuracy evaluation methods.\n\nLets first load required libraries:","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### About dataset","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"This dataset is about past loans. The __Loan_train.csv__ data set includes details of 346 customers whose loan are already paid off or defaulted. It includes following fields:\n\n| Field          | Description                                                                           |\n|----------------|---------------------------------------------------------------------------------------|\n| Loan_status    | Whether a loan is paid off on in collection                                           |\n| Principal      | Basic principal loan amount at the                                                    |\n| Terms          | Origination terms which can be weekly (7 days), biweekly, and monthly payoff schedule |\n| Effective_date | When the loan got originated and took effects                                         |\n| Due_date       | Since it’s one-time payoff schedule, each loan has one single due date                |\n| Age            | Age of applicant                                                                      |\n| Education      | Education of applicant                                                                |\n| Gender         | The gender of applicant                                                               |","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Lets download the dataset","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"!wget -O loan_train.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data From CSV File  ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df = pd.read_csv('loan_train.csv')\ndf.head()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert to date time object ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df['due_date'] = pd.to_datetime(df['due_date'])\ndf['effective_date'] = pd.to_datetime(df['effective_date'])\ndf.head()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualization and pre-processing\n\n","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Let’s see how many of each class is in our data set ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df['loan_status'].value_counts()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"260 people have paid off the loan on time while 86 have gone into collection \n","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Lets plot some columns to underestand data better:","metadata":{}},{"cell_type":"code","source":"# notice: installing seaborn might takes a few minutes\n!conda install -c anaconda seaborn -y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nbins = np.linspace(df.Principal.min(), df.Principal.max(), 10)\ng = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\ng.map(plt.hist, 'Principal', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins = np.linspace(df.age.min(), df.age.max(), 10)\ng = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\ng.map(plt.hist, 'age', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing:  Feature selection/extraction","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"### Lets look at the day of the week people get the loan ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df['dayofweek'] = df['effective_date'].dt.dayofweek\nbins = np.linspace(df.dayofweek.min(), df.dayofweek.max(), 10)\ng = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\ng.map(plt.hist, 'dayofweek', bins=bins, ec=\"k\")\ng.axes[-1].legend()\nplt.show()\n","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that people who get the loan at the end of the week dont pay it off, so lets use Feature binarization to set a threshold values less then day 4 ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\ndf.head()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert Categorical features to numerical values","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Lets look at gender:","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"86 % of female pay there loans while only 73 % of males pay there loan\n","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Lets convert male to 0 and female to 1:\n","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ndf.head()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One Hot Encoding  \n#### How about education?","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df.groupby(['education'])['loan_status'].value_counts(normalize=True)","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature befor One Hot Encoding","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"df[['Principal','terms','age','Gender','education']].head()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use one hot encoding technique to conver categorical varables to binary variables and append them to the feature Data Frame ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"Feature = df[['Principal','terms','age','Gender','weekend']]\nFeature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)\nFeature.drop(['Master or Above'], axis = 1,inplace=True)\nFeature.head()\n","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature selection","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Lets defind feature sets, X:","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"X = Feature\nX[0:5]","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What are our lables?","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"y = df['loan_status'].values\ny[0:5]","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize Data ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Data Standardization give data zero mean and unit variance (technically should be done after train test split )","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"X= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Now, it is your turn, use the training set to build an accurate model. Then use the test set to report the accuracy of the model\nYou should use the following algorithm:\n- K Nearest Neighbor(KNN)\n- Decision Tree\n- Support Vector Machine\n- Logistic Regression\n\n\n\n__ Notice:__ \n- You can go above and change the pre-processing, feature selection, feature-extraction, and so on, to make a better model.\n- You should use either scikit-learn, Scipy or Numpy libraries for developing the classification algorithms.\n- You should include the code of the algorithm in the following cells.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"# K Nearest Neighbor(KNN)\nNotice: You should find the best k to build the model with the best accuracy.  \n**warning:** You should not use the __loan_test.csv__ for finding the best k, however, you can split your train_loan.csv into train and test to find the best __k__.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nk = 6\n\nneighK6 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneighK6\n \nyhat = neighK6.predict(X_test)\nyhat[0:5]\n\nfrom sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neighK6.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()\nprint( \"Best accuracy:\", mean_acc.max(), \"k=\", mean_acc.argmax()+1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)\n#Modelling\nTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6)\nTree","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tree.fit(X_trainset,y_trainset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predTree = Tree.predict(X_testset)\nprint (predTree [0:5])\nprint (y_testset [0:5])\n\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nprint(\"Accuracy: \", metrics.accuracy_score(y_testset, predTree))\n\n\n!conda install -c conda-forge pydotplus -y\n!conda install -c conda-forge python-graphviz -y\nfrom sklearn.externals.six import StringIO\nimport pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\n%matplotlib inline \ndot_data = StringIO()\nfilename = \"loan.png\"\nfeatureNames = df.columns[0:8]\ntargetNames = df['loan_status'].unique().tolist()\nout=tree.export_graphviz(Tree,feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_trainset), filled=True,  special_characters=True,rotate=False)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(filename)\nimg = mpimg.imread(filename)\nplt.figure(figsize=(100, 200))\nplt.imshow(img,interpolation='nearest')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine","metadata":{}},{"cell_type":"code","source":"df.dtypes\ndf = df[pd.to_numeric(df['education'], errors='coerce').notnull()]\ndf['education'] = df['education'].astype('int')\ndf.dtypes\n\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat = clf.predict(X_test)\nyhat [0:5]\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\ncnf_matrix = confusion_matrix(y_test, yhat, labels=['PAIDOFF','COLLECTION'])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat))\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')\n\nfrom sklearn.metrics import f1_score\nf1_score(y_test, yhat, average='weighted')\n\nfrom sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"df = df[['loan_status', 'Principal', 'terms', 'effective_date', 'due_date', 'age', 'education', 'Gender']]\ndf['loan_status'] = df['loan_status'].astype('int')\n\nfrom sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLogR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLogR","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat = LogR.predict(X_test)\nyhat\nyhat_prob = LogR.predict_proba(X_test)\nyhat_prob","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation using Test set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, download and load the test set:","metadata":{}},{"cell_type":"code","source":"!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Test set for evaluation ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"test_df = pd.read_csv('loan_test.csv')\ntest_df.head()","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\nY = test_df['loan_status'].values\nY[0:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test the KNN algorithm already trained with K=6\nyhatKNN=neigh.predict(X)\nKNNJaccard = jaccard_similarity_score(y, yhatKNN)\nKNNF1 = f1_score(y, yhatKNN, average='weighted')\nprint(\"Avg F1-score: %.2f\" % KNNF1 )\nprint(\"KNN Jaccard Score: %.2f\" % KNNJaccard)\n\n\nyhatDEC = Tree.predict(X)\nDTJaccard = jaccard_similarity_score(y, yhatDEC)\nDTF1 = f1_score(y, yhatDEC, average='weighted')\nprint(\"Avg F1-score: %.2f\" % DTF1 )\nprint(\"Decision Tree Jaccard Score: %.2f\" % DTJaccard)\n\nyhatSVM=clf.predict(X)\nSVMJaccard = jaccard_similarity_score(y, yhatSVM)\nSVMF1 = f1_score(y, yhatSVM, average='weighted')\nprint(\"Avg F1-score: %.2f\" % SVMF1)\nprint(\"SVM Jaccard score: %.2f\" % SVMJaccard)\n\nyhatLOG = LogR.predict(X)\nyhatLOGproba = LogR.predict_proba(X)\nLogRJaccard = jaccard_similarity_score(y, yhatLOG)\nLogRF1 = f1_score(y, yhatLOG, average='weighted')\nLogloss = log_loss(y, yhatLOGproba)\nprint(\"LogLoss: : %.2f\" % Logloss)\nprint(\"Avg F1-score: %.4f\" % LogRF1)\nprint(\"LOG Jaccard score: %.4f\" % LogRJaccard)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Report\nYou should be able to report the accuracy of the built model using different evaluation metrics:","metadata":{}},{"cell_type":"markdown","source":"| Algorithm          | Jaccard | F1-score | LogLoss |\n|--------------------|---------|----------|---------|\n| KNN                | 0.79    | 0.78     | NA      |\n| Decision Tree      | 0.79    | 0.78     | NA      |\n| SVM                | 0.77    | 0.76     | NA      |\n| LogisticRegression | 0.7428  | 0.7199   | 0.56    |","metadata":{}}]}